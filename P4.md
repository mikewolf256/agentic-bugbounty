
# P4 — Commercial Red-Team Platform Roadmap

## ✅ P4.1 — MITRE ATT&CK Mapping Engine
- [ ] Implement LLM-based MITRE mapping for each finding
- [ ] Create static mapping rules for common bug classes
- [ ] Add `"mitre"` field to triage JSON schema
- [ ] Store MITRE tags under `artifacts/mitre/`
- [ ] Integrate MITRE tags with RAG memory

## ✅ P4.2 — ATT&CK Navigator Export
- [ ] Create `export_mitre_navigator.py`
- [ ] Generate valid Navigator JSON output
- [ ] Color-code techniques by severity
- [ ] Add evidence links in technique comments
- [ ] Implement `/mcp/export_mitre` endpoint

## ✅ P4.3 — Executive Red-Team PDF Report
- [ ] Build HTML → PDF report pipeline
- [ ] Write Jinja2 template (exec summary, tech details, mitigation)
- [ ] Add MITRE matrix visualization to the PDF
- [ ] Add finding impact scoring summary
- [ ] Implement `/mcp/export_redteam_report` API

## ✅ P4.4 — Continuous ASM (Attack Surface Monitoring)
- [ ] Add scheduler for recurring scans (daily/weekly)
- [ ] Add Slack/Email/Teams alerting
- [ ] Add multi-tenant workspace separation
- [ ] Add global token-usage monitor + alerts
- [ ] Prototype “client portal” (future UI)

## ✅ P4.5 — Red-Team Simulation Mode
- [ ] Add vulnerability chaining logic
- [ ] Add role-diff privilege escalation tester (auth required)
- [ ] Add JWT-scope analyzer
- [ ] Add attack graph generator (JSON)
- [ ] Add “high-value-path” scoring system (kill chain probability)

---
You’re in a _really_ good place for a full-stack test run now. I’d validate things in layers, roughly following your README’s flow but with some extra P0-specific checks.

I’ll lay this out like a test checklist you can literally paste into an Obsidian note.

---

## 1. Pre-flight sanity checks

**Goal:** Make sure all plumbing is there before doing “real” scans.

-  **Python env + deps**
    
    - `source .venv/bin/activate`
        
    - `pip install -r requirements.txt`
        
-  **Env vars set**
    
    - `OPENAI_API_KEY`
        
    - (Optional) `ZAP_API_BASE`, `ZAP_API_KEY`, `OUTPUT_DIR` (defaults to `./output_zap`)
        
-  **External tools on PATH**
    
    - `which zap.sh` (or Docker ZAP reachable at the URL you configured)
        
    - `which ffuf`
        
    - `which dalfox`
        
    - `which nuclei`
        
-  **Output dir is writeable**
    
    - `ls output_zap` and confirm no permission errors.
        

---

## 2. Scope + MCP basic flow

**Goal:** Prove the control plane works end-to-end.

1. **Start MCP server**
    
    - `python mcp_zap_server.py`
        
    - Hit `http://127.0.0.1:8000/mcp/health` and confirm `{"status":"ok"}` (or similar).
        
2. **Load a simple scope**
    
    ```bash
    cat > scope.json << 'EOF'
    {
      "program_name": "demo-program",
      "primary_targets": [
        "https://app.example.com",
        "https://api.example.com"
      ],
      "secondary_targets": [],
      "rules": {}
    }
    EOF
    
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/set_scope \
      -H 'Content-Type: application/json' \
      --data-binary @scope.json | jq .
    ```
    
    -  Verify response shows `"program": "demo-program"`.
        
3. **Negative test:** send invalid JSON or missing field to `/mcp/set_scope` and confirm a clean error response (400 with message, not a stack trace).
    

---

## 3. ZAP scanning + export path

**Goal:** Prove ZAP ↔ MCP ↔ triage pipeline all work together.

1. **Start ZAP**  
    e.g. via Docker or local daemon, making sure `ZAP_API_BASE` in your env matches where it’s listening.
    
2. **Kick off a scan via MCP**
    
    ```bash
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/start_zap_scan \
      -H 'Content-Type: application/json' \
      -d '{"targets": ["https://app.example.com", "https://api.example.com"]}' | jq .
    ```
    
    -  Capture the returned `scan_id`.
        
3. **Poll / check alerts**
    
    - Depending on how you wired it, either:
        
        - Have your poller export to `output_zap/zap_findings_<scan_id>.json`, or
            
        - Call whatever `/mcp/*` endpoint you built to dump alerts.
            
    -  Confirm `output_zap/zap_findings_<scan_id>.json` exists and is valid JSON.
        
4. **Run AI triage on that findings file**
    
    ```bash
    python agentic_from_file.py \
      --findings_file output_zap/zap_findings_<scan_id>.json \
      --scope_file scope.json
    ```
    
    -  Confirm `output_zap/triage_<scan_id>.json` is created.
        
    -  Confirm at least one per-finding markdown like `output_zap/<scan_id>__*.md`.
        
5. **Export via MCP**
    
    ```bash
    curl -s \
      -X GET http://127.0.0.1:8000/mcp/export_report/<scan_id> | jq .
    ```
    
    -  Confirm:
        
        - `output_zap/<scan_id>_reports_index.json`
            
        - `output_zap/<scan_id>_<finding_id>.md`  
            (even if dedupe filtered 90% of noise).
            
6. **Spot-check one report**
    
    -  It has title, CVSS, summary, repro, impact, remediation.
        
    -  Scope disclaimer and research header line are present.
        

---

## 4. P0 feature-specific checks

### 4.1 Dedupe + pre-LLM gating

**Goal:** Confirm low-value findings are getting dropped before AI.

1. Create a tiny synthetic findings file with:
    
    - Some obvious “noise” issues (informational, low risk, no evidence).
        
    - One higher-risk one (e.g. XSS-ish or “SQL Injection” wording).
        
2. Run `agentic_from_file.py` against it.
    
3. Check `output_zap/triage_<id>.json`:
    
    -  Noise items are missing or marked with `confidence: "low"` and `recommended_bounty_usd: 0`.
        
    -  Higher-risk item survives and has a non-zero CVSS.
        

You can also add a log in `mcp_helpers/dedupe.py` (if not already) to log how many findings were dropped vs kept.

---

### 4.2 Dalfox XSS validation

**Goal:** Prove the “extra confirmation” and confidence bump actually happen.

1. Use the existing `output_zap/test_findings.json` you already used (with a fake XSS).
    
2. Run:
    
    ```bash
    OPENAI_API_KEY=... DALFOX_BIN=$(which dalfox) PYTHONUNBUFFERED=1 \
    python agentic_from_file.py \
      --findings_file output_zap/test_findings.json \
      --scope_file scope.json
    ```
    
3. Inspect:
    
    ```bash
    jq '.[0].validation' output_zap/triage_test_findings.json
    ```
    
    -  Contains `dalfox` object with `engine_result` and maybe `payload`.
        
    -  `dalfox_confirmed` field is present (`true` or `false`).
        
4. Compare `confidence` and `recommended_bounty_usd` versus a run where Dalfox is disabled or fails:
    
    -  On a confirmed finding, confidence is bumped from `low` → `medium` and bounty is > 0.
        

---

### 4.3 JS / config miner

**Goal:** Make sure you’re extracting endpoints from JS / JSON and writing artifacts correctly.

1. Run directly (you already did with `example.com`):
    
    ```bash
    PYTHONPATH=. python tools/js_miner.py \
      --base-url https://you.23andme.com \
      --output output_zap/artifacts/js_miner/you.23andme.com
    ```
    
2. Verify:
    
    -  `endpoints.json` exists.
        
    -  It contains at least some paths / URLs (not just `[]`).
        
3. Optional:
    
    -  Check that obvious JS files from the page are referenced.
        

If JS miner is also wired through an MCP endpoint (e.g. `/mcp/run_js_miner`), hit it once and ensure it writes to the same `artifacts/js_miner/<host>/` layout.

---

### 4.4 Backup / VCS hunt (ffuf playbook)

**Goal:** Confirm ffuf integration and artifact structure.

1. Run your helper:
    
    ```bash
    PYTHONPATH=. python tools/backup_hunt.py \
      --target https://app.example.com \
      --output output_zap/artifacts/backup_hunt/app.example.com
    ```
    
2. Check:
    
    -  `ffuf_<timestamp>.json` exists under that directory.
        
    -  File is valid JSON (even if zero matches).
        
    -  If you test against a controlled target with a fake `/.git/` or `/backup.sql`, ffuf picks it up.
        

Same idea if there’s an MCP wrapper endpoint (`/mcp/run_ffuf`): call it and confirm identical artifact behavior.

---

### 4.5 Reflector tester

**Goal:** See that “blank/error page → param fuzz → reflection” logic works.

1. Point `reflector_tester` at a target that’s known to reflect query params (or a local test app if you have one).
    
    ```bash
    PYTHONPATH=. python tools/reflector_tester.py \
      --url 'https://example.com/?q=test' \
      --output output_zap/artifacts/reflector/example.com
    ```
    
2. Check:
    
    -  It prints `Reflection findings: [...]` with at least one entry in a reflective case.
        
    -  The findings file in the artifacts directory (if you’re writing one) is valid JSON and lists parameters/urls.
        

If there’s an MCP `/mcp/run_reflector` endpoint, run it once for a known host and confirm the response returns the same list.

---

## 5. Nuclei recon + PoC validation

**Goal:** Validate the new nuclei MCP endpoints.

1. **Recon mode**
    
    ```bash
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/run_nuclei \
      -H 'Content-Type: application/json' \
      -d '{
            "target": "https://api.example.com",
            "mode": "recon"
          }' | jq .
    ```
    
    -  Response indicates the command that ran and output path.
        
    -  Nuclei output exists under `output_zap/nuclei/...` (or whatever you configured).
        
2. **PoC validation**
    
    - Pick a simple nuclei template (even a benign one like a technology fingerprint).
        
    
    ```bash
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/validate_poc_with_nuclei \
      -H 'Content-Type: application/json' \
      -d '{
            "target": "https://api.example.com",
            "templates": ["http/technologies/tech-detect.yaml"]
          }' | jq .
    ```
    
    -  Response includes `validated`, `match_count`, `findings`, `summaries`.
        
    -  For a known-positive template, `validated` should be `true` or `match_count > 0`.
        

---

## 6. Host profile aggregation

**Goal:** Make sure your “LLM view” is building a compact, useful summary.

1. After at least one ZAP scan + nuclei recon run:
    
    ```bash
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/host_profile \
      -H 'Content-Type: application/json' \
      -d '{"host": "https://api.example.com", "llm_view": true}' | jq .
    ```
    
2. Check:
    
    -  Response includes endpoint list, vuln summary, and nuclei bits.
        
    -  `llm_profile` exists and is not gigantic (a few KB, not megabytes).
        
    -  No secrets or Authorization headers are present in the profile.
        

---

## 7. Auth plumbing smoke test (if you have any tokenized test target)

Even if the target doesn’t really need auth yet, just smoke-test the wiring:

1. Configure:
    
    ```bash
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/set_auth \
      -H 'Content-Type: application/json' \
      -d '{
            "host": "app.example.com",
            "type": "header",
            "headers": {"Authorization": "Bearer DUMMY_TOKEN"}
          }' | jq .
    ```
    
2. Run:
    
    ```bash
    curl -s \
      -X POST http://127.0.0.1:8000/mcp/start_auth_scan \
      -H 'Content-Type: application/json' \
      -d '{"targets":["https://app.example.com"]}' | jq .
    ```
    
3. Verify:
    
    -  The scan is accepted and started.
        
    -  Logs show the Authorization header being used _and_ redacted in stored artifacts/logs (e.g. `Bearer ****`).
        

---

## 8. Error handling & resilience

**Goal:** Make sure the system fails gracefully when bits are missing.

Pick a few of these:

-  Stop ZAP and try `/mcp/start_zap_scan` → should get a clean “Error contacting ZAP” JSON, not a traceback.
    
-  Temporarily unset `OPENAI_API_KEY` and run `agentic_from_file.py` → should exit with your friendly “Set OPENAI_API_KEY env var.” message.
    
-  Run `/mcp/run_nuclei` with an invalid template path → explicit error in JSON, no partial files left behind.
    

---

If you walk through all of that and it behaves, you’ve basically validated:

- MCP endpoints
    
- ZAP integration
    
- Deduping + CVSS gating
    
- Dalfox validator
    
- Recon helpers (JS miner, backup hunt, reflector)
    
- Nuclei recon & PoC validation
    
- Report export
    

If you want, next step after this test pass could be to add a `tests/` bash script or Python smoke-test harness that automates a subset of these with a simple local test target, so you can re-run them before any big refactor or before you scale to containers.